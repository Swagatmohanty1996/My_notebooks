{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing the positive and Negative words from Loughran and McDonald Dictionary\n",
    "\n",
    "def loadPositive():\n",
    "\n",
    "\n",
    "    myfile= open('/apps/apps1/a625533/Positive_words .csv',\"r\")\n",
    "    positives = myfile.readlines()\n",
    "    positive = [pos.strip().lower() for pos in positives]\n",
    "#     print(positive)\n",
    "    return positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadNegative():\n",
    "\n",
    "\n",
    "    yfile= open('/apps/apps1/a625533/Negative_words .csv',\"r\")\n",
    "    negatives = yfile.readlines()\n",
    "    negative = [neg.strip().lower() for neg in negatives]\n",
    "# print(negative)\n",
    "\n",
    "    return negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to get the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to count the number of words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "    \n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Funtion to parse out MDA Section which is present in  Item 7 of a SEC Filing\n",
    "\n",
    "def parse_mda(text, start=0):\n",
    "    debug = False\n",
    "#     \"\"\"Parse normalized text \n",
    "#     \"\"\"\n",
    "\n",
    "    mda = \"\"\n",
    "    end = 0\n",
    "#     \"\"\"\n",
    "#         Parsing Rules\n",
    "#     \"\"\"\n",
    "\n",
    "    # Define start & end signal for parsing\n",
    "    item7_begins = [\n",
    "        '\\nItem 7.', '\\nItem 7 –', '\\nItem 7:', '\\nItem 7 ', '\\nItem 7\\n' \n",
    "    ]\n",
    "    item7_ends = ['\\nItem 7A']\n",
    "    if start != 0:\n",
    "        item7_ends.append('\\nItem 7')  # Case: ITEM 7A does not exist\n",
    "    item8_begins = ['\\nItem 8']\n",
    "#     \"\"\"\n",
    "#         Parsing code section\n",
    "#     \"\"\"\n",
    "    text = text[start:]\n",
    "\n",
    "    # Get begin\n",
    "    for item7 in item7_begins:\n",
    "        begin = text.find(item7)\n",
    "        if debug:\n",
    "            print(item7, begin)\n",
    "        if begin != -1:\n",
    "            break\n",
    "\n",
    "    if begin != -1:  # Begin found\n",
    "        for item7A in item7_ends:\n",
    "            end = text.find(item7A, begin + 1)\n",
    "            if debug:\n",
    "                print(item7A, end)\n",
    "            if end != -1:\n",
    "                break\n",
    "\n",
    "        if end == -1:  # ITEM 7A does not exist\n",
    "            for item8 in item8_begins:\n",
    "                end = text.find(item8, begin + 1)\n",
    "                if debug:\n",
    "                    print(item8, end)\n",
    "                if end != -1:\n",
    "                    break\n",
    "\n",
    "        # Get MDA\n",
    "        if end > begin:\n",
    "            mda = text[begin:end].strip()\n",
    "        else:\n",
    "            end = 0\n",
    "\n",
    "    return mda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Funtion to parse out Risk Factor Section which is present in  Item 1A of a SEC Filing\n",
    "\n",
    "def parse_rf(text, start=0):\n",
    "    debug = False\n",
    "#     \"\"\"Parse normalized text \n",
    "#     \"\"\"\n",
    "\n",
    "    rf = \"\"\n",
    "    end = 0\n",
    "#     \"\"\"\n",
    "#         Parsing Rules\n",
    "#     \"\"\"\n",
    "\n",
    "    # Define start & end signal for parsing\n",
    "    item1_begins = [\n",
    "        '\\nItem 1A.', '\\nItem 1A –', '\\nItem 1A:', '\\nItem 1A ', '\\nItem 1A\\n' \n",
    "    ]\n",
    "    item1_ends = ['\\nItem 1B']\n",
    "    if start != 0:\n",
    "        item1_ends.append('\\nItem 1A')  # Case: ITEM 7A does not exist\n",
    "    item2_begins = ['\\nItem 2']\n",
    "#     \"\"\"\n",
    "#         Parsing code section\n",
    "#     \"\"\"\n",
    "    text = text[start:]\n",
    "\n",
    "    # Get begin\n",
    "    for item1 in item1_begins:\n",
    "        begin = text.find(item1)\n",
    "        if debug:\n",
    "            print(item1, begin)\n",
    "        if begin != -1:\n",
    "            break\n",
    "\n",
    "    if begin != -1:  # Begin found\n",
    "        for item1A in item1_ends:\n",
    "            end = text.find(item1A, begin + 1)\n",
    "            if debug:\n",
    "                print(item1A, end)\n",
    "            if end != -1:\n",
    "                break\n",
    "\n",
    "        if end == -1:  # ITEM 1A does not exist\n",
    "            for item2 in item2_begins:\n",
    "                end = text.find(item2, begin + 1)\n",
    "                if debug:\n",
    "                    print(item2, end)\n",
    "                if end != -1:\n",
    "                    break\n",
    "\n",
    "        # Get Risk Factor\n",
    "        if end > begin:\n",
    "            rf = text[begin:end].strip()\n",
    "        else:\n",
    "            end = 0\n",
    "\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to group the date to date stock price based on year (June-May)\n",
    "\n",
    "def grouping(start_year,date_year,date_month,date_day):\n",
    "    if date_month<6:\n",
    "        group=date_year-start_year\n",
    "    else:\n",
    "        group=date_year-start_year+1\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to group as per the quarter\n",
    "\n",
    "def group_quarter(start_year,date_year,date_month,date_day):\n",
    "    if(date_month<3):\n",
    "        group=(date_year-start_year)*4 - 1\n",
    "    elif date_month>=3 and date_month<6:\n",
    "        group=(date_year - start_year)*4\n",
    "    elif date_month>=6 and date_month<9:\n",
    "        group= (date_year - start_year)*4 + 1\n",
    "    elif date_month>=9 and date_month<12:\n",
    "        group=  (date_year - start_year)*4 + 2\n",
    "    else:\n",
    "        group= (date_year - start_year)*4 + 3\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function which says which quarter of that year the particular date lies\n",
    "\n",
    "def Number_quarter(start_year,date_year,date_month,date_day):\n",
    "    if(date_month==3):\n",
    "        Quarter_Number=\"Q4\"\n",
    "    elif date_month==12:\n",
    "        Quarter_Number=\"Q3\"\n",
    "    elif date_month==6:\n",
    "        Quarter_Number=\"Q1\"\n",
    "#     elif date_month==9:\n",
    "#         Quarter_Number=\"Q2\"\n",
    "    else:\n",
    "        Quarter_Number=\"Q2\"\n",
    "    return Quarter_Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The defination of year as per us is June of a particular year to May of the subsequent year . This function helps to categorize the stock price of the last quarter under the previous year . \n",
    "#Suppose for the year 2016\n",
    "#the four quarter are June'16-Aug'16 , Sept'16-Nov'16 ,Dec'16-Feb'17 ,Mar'17-May'17\n",
    "#This function helps us categorize Mar'17-May'17 also under the year 2016 .\n",
    "\n",
    "def Year(start_year,date_year,date_month,date_day):\n",
    "    if(date_month==3):\n",
    "        year=date_year-1\n",
    "    else:\n",
    "        year=date_year\n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import edgar\n",
    "from __future__ import division, print_function\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import re, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import requests  # functions for interacting with web pages\n",
    "from bs4 import BeautifulSoup  # DOM html manipulation\n",
    "from pyparsing import (makeHTMLTags, SkipTo, commonHTMLEntity, replaceHTMLEntity, \n",
    "    htmlComment, anyOpenTag, anyCloseTag, LineEnd, OneOrMore, replaceWith)\n",
    "from pyparsing import ParserElement\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "\n",
    "#we are taking the company ticker ,company name and the central index number as input \n",
    "def edgar_data(Company_ticker, CompanyName, CIK_Number):\n",
    "        #Extracting 10-k Documents of a company\n",
    "        company = edgar.Company(CompanyName, CIK_Number)\n",
    "        tree = company.getAllFilings(filingType = \"10-K\")\n",
    "        # tree = company.getFilingsUrl(filingType = \"10-K\")\n",
    "        docs = edgar.getDocuments(tree, noOfDocuments=8)\n",
    "        \n",
    "        #We are extracting edgar filing of the company for the last 8 years\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({'text':docs})\n",
    "\n",
    "\n",
    "        #Formatting the whole Text\n",
    "\n",
    "        FormatText=[]\n",
    "        for text in docs:\n",
    "\n",
    "            text = unicodedata.normalize(\"NFKD\", text)  # Normalize\n",
    "            text = '\\n'.join(\n",
    "                text.splitlines())  # Let python take care of unicode break lines\n",
    "\n",
    "            # Convert to upper\n",
    "        #     text = text.upper()  # Convert to upper\n",
    "\n",
    "            # Take care of breaklines & whitespaces combinations due to beautifulsoup parsing\n",
    "            text = re.sub(r'[ ]+\\n', '\\n', text)\n",
    "            text = re.sub(r'\\n[ ]+', '\\n', text)\n",
    "            text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "            # To find MDA section, reformat item headers\n",
    "            text = text.replace('\\n.\\n', ' ')  # Move Period to beginning\n",
    "\n",
    "        # #     text = text.replace('\\nI\\nTEM', '\\nITEM')\n",
    "        #     text = text.replace('\\nI\\ntem', '\\nItem')\n",
    "        # #     text = text.replace('\\nITEM\\n', '\\nITEM ')\n",
    "        #     text= text.replace('\\nItem\\n' ,'\\nItem')\n",
    "        # #     text = text.replace('\\nITEM  ', '\\nITEM ')\n",
    "        #     text= text.replace('\\nItem' , '\\nItem')\n",
    "\n",
    "            text = text.replace(':\\n', ' ')\n",
    "\n",
    "            # Math symbols for clearer looks\n",
    "            text = text.replace('$\\n', '$')\n",
    "            text = text.replace('\\n%', '%')\n",
    "            text = text.replace('\\n', ' ')\n",
    "\n",
    "        #     # Reformat\n",
    "        #     text = text.replace('\\n', '\\n\\n')  # Reformat by additional breakline\n",
    "\n",
    "            FormatText.append(text)\n",
    "\n",
    "        df['format_text']=FormatText\n",
    "\n",
    "        #Find out Jaccard Similarity for the entire document\n",
    "\n",
    "        jaccard_Whole_Text=['NA']\n",
    "\n",
    "        str=[FormatText[0],FormatText[1],FormatText[2],FormatText[3],FormatText[4],FormatText[5],FormatText[6],FormatText[7]]\n",
    "        for i in range(0,7):\n",
    "\n",
    "            a = set(str[i%8].split())\n",
    "            b = set(str[(i+1)%8].split())\n",
    "            c = a.intersection(b)\n",
    "            sim_jaccard_method= float(len(c)) / (len(a) + len(b) - len(c))\n",
    "            jaccard_Whole_Text.append(sim_jaccard_method)\n",
    "\n",
    "        df['jaccard_similarity_whole_text ']=jaccard_Whole_Text\n",
    "\n",
    "        \n",
    "\n",
    "        WORD = re.compile(r'\\w+')\n",
    "\n",
    "\n",
    "\n",
    "        #Cosine Similarity of the entire document for two consecutive years\n",
    "\n",
    "        Cosine_FullText=['NA']\n",
    "        str=[FormatText[0],FormatText[1],FormatText[2],FormatText[3],FormatText[4],FormatText[5],FormatText[6],FormatText[7]]\n",
    "\n",
    "        for i in range(0,7):\n",
    "\n",
    "\n",
    "            vector1 = text_to_vector(str[i%8])\n",
    "            vector2 = text_to_vector(str[(i+1)%8])\n",
    "\n",
    "            cosine = get_cosine(vector1, vector2)\n",
    "            Cosine_FullText.append(cosine)\n",
    "\n",
    "        df['cosine_similarity_whole_text']=Cosine_FullText\n",
    "\n",
    "        #Finding out positive and Negative words in the document\n",
    "\n",
    "        Count_positive=[]\n",
    "        Count_negative=[]\n",
    "        diff=[]\n",
    "        for text in FormatText:\n",
    "\n",
    "\n",
    "            \n",
    "            positive_number=loadPositive()\n",
    "            negative_number=loadNegative()\n",
    "            \n",
    "            # count = Counter(paragraph.split())\n",
    "            count = Counter(text.split())\n",
    "            #print(count.items())\n",
    "\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            for key, val in count.items():\n",
    "                key = key.rstrip('.,?!\\n') # removing possible punctuation signs\n",
    "                if key in positive_number:\n",
    "                    pos += val\n",
    "                if key in negative_number:\n",
    "                    neg += val\n",
    "            Difference=neg-pos\n",
    "            Count_positive.append(pos)\n",
    "            Count_negative.append(neg)\n",
    "            diff.append(Difference)\n",
    "\n",
    "        df['count_positive_fulltext']=Count_positive\n",
    "        df['count_negative_fulltext']=Count_negative\n",
    "        df['difference_fulltext']=diff\n",
    "\n",
    "        #To find total number of words in the document\n",
    "\n",
    "        Total_words=[]\n",
    "        for fname in FormatText:\n",
    "\n",
    "            num_words = 0\n",
    "\n",
    "            for line in fname:\n",
    "                words = line.split()\n",
    "                num_words += len(words)\n",
    "            Total_words.append(num_words)\n",
    "\n",
    "        df['total_words_fulltext']=Total_words\n",
    "\n",
    "        #To find out pessimism = (Negative-Positive)/Total number of words\n",
    "\n",
    "        df['pessimism']=df['difference_fulltext']/df['total_words_fulltext']\n",
    "\n",
    "        #Change in Pessimism over last year\n",
    "\n",
    "        df['change_in_pessimism_over_last_year'] = df['pessimism'] - df['pessimism'].shift(-1)\n",
    "\n",
    "        #To parse out MDA\n",
    "\n",
    "        NewText=[]\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "\n",
    "        #             start = 'Changes in and Disagreements with Accountants on Accounting and Financial Disclosure'\n",
    "#                     start = 'Changes in and Disagreements with Accountants on Accounting and Financial'\n",
    "#                     start = 'Changes in and Disagreements with Accountants on'\n",
    "                    start = 'Changes in and Disagreements'\n",
    "#                     start = 'Disagreements with'\n",
    "                    s = row['text']\n",
    "                    new1=s.split(start)[1]\n",
    "        #             print(new1)\n",
    "                    NewText.append(new1)\n",
    "\n",
    "        df['new_text']=NewText\n",
    "\n",
    "        #Formatting the text to remove new line\n",
    "\n",
    "        FormatText_New=[]\n",
    "        for text in NewText:\n",
    "\n",
    "            text = unicodedata.normalize(\"NFKD\", text)  # Normalize\n",
    "            text = '\\n'.join(\n",
    "                text.splitlines())  # Let python take care of unicode break lines\n",
    "\n",
    "            # Convert to upper\n",
    "        #     text = text.upper()  # Convert to upper\n",
    "\n",
    "            # Take care of breaklines & whitespaces combinations due to beautifulsoup parsing\n",
    "            text = re.sub(r'[ ]+\\n', '\\n', text)\n",
    "            text = re.sub(r'\\n[ ]+', '\\n', text)\n",
    "            text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "            # To find MDA section, reformat item headers\n",
    "            text = text.replace('\\n.\\n', '.\\n')  # Move Period to beginning\n",
    "\n",
    "        #     text = text.replace('\\nI\\nTEM', '\\nITEM')\n",
    "            text = text.replace('\\nI\\ntem', '\\nItem')\n",
    "        #     text = text.replace('\\nITEM\\n', '\\nITEM ')\n",
    "            text= text.replace('\\nItem\\n' ,'\\nItem')\n",
    "        #     text = text.replace('\\nITEM  ', '\\nITEM ')\n",
    "            text= text.replace('\\nItem' , '\\nItem')\n",
    "\n",
    "            text = text.replace(':\\n', '.\\n')\n",
    "\n",
    "            # Math symbols for clearer looks\n",
    "            text = text.replace('$\\n', '$')\n",
    "            text = text.replace('\\n%', '%')\n",
    "\n",
    "            # Reformat\n",
    "            text = text.replace('\\n', '\\n\\n')  # Reformat by additional breakline\n",
    "\n",
    "            FormatText_New.append(text)\n",
    "\n",
    "        mda_list=[]\n",
    "        for text in FormatText_New:\n",
    "            mda = parse_mda(text)\n",
    "            mda_list.append(mda)\n",
    "        #     print(type(mda))\n",
    "            # Parse second time if first parse results in index\n",
    "            if mda and len(mda.encode('utf-8')) < 1000:\n",
    "                mda= parse_mda(text, start=end)\n",
    "\n",
    "        #Format MDA part\n",
    "\n",
    "        MDA_FormatText=[]\n",
    "        for text in mda_list:\n",
    "\n",
    "            text = unicodedata.normalize(\"NFKD\", text)  # Normalize\n",
    "            text = '\\n'.join(\n",
    "                text.splitlines())  # Let python take care of unicode break lines\n",
    "\n",
    "            # Convert to upper\n",
    "        #     text = text.upper()  # Convert to upper\n",
    "\n",
    "            # Take care of breaklines & whitespaces combinations due to beautifulsoup parsing\n",
    "            text = re.sub(r'[ ]+\\n', '\\n', text)\n",
    "            text = re.sub(r'\\n[ ]+', '\\n', text)\n",
    "            text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "            # To find MDA section, reformat item headers\n",
    "            text = text.replace('\\n.\\n', ' ')  # Move Period to beginning\n",
    "\n",
    "        # #     text = text.replace('\\nI\\nTEM', '\\nITEM')\n",
    "        #     text = text.replace('\\nI\\ntem', '\\nItem')\n",
    "        # #     text = text.replace('\\nITEM\\n', '\\nITEM ')\n",
    "        #     text= text.replace('\\nItem\\n' ,'\\nItem')\n",
    "        # #     text = text.replace('\\nITEM  ', '\\nITEM ')\n",
    "        #     text= text.replace('\\nItem' , '\\nItem')\n",
    "\n",
    "            text = text.replace(':\\n', ' ')\n",
    "\n",
    "            # Math symbols for clearer looks\n",
    "            text = text.replace('$\\n', '$')\n",
    "            text = text.replace('\\n%', '%')\n",
    "            text = text.replace('\\n', ' ')\n",
    "\n",
    "        #     # Reformat\n",
    "        #     text = text.replace('\\n', '\\n\\n')  # Reformat by additional breakline\n",
    "\n",
    "            MDA_FormatText.append(text)\n",
    "\n",
    "        df['mda_formattext']=MDA_FormatText\n",
    "\n",
    "        #Find Jaccard Similarity for the formatted MDA Section\n",
    "\n",
    "        #Formatted MDA Jaccard Similarity\n",
    "        jaccard_MDA=['NA']\n",
    "\n",
    "        str=[MDA_FormatText[0],MDA_FormatText[1],MDA_FormatText[2],MDA_FormatText[3],MDA_FormatText[4],MDA_FormatText[5],MDA_FormatText[6],MDA_FormatText[7]]\n",
    "        for i in range(0,7):\n",
    "\n",
    "            a = set(str[i%8].split())\n",
    "            b = set(str[(i+1)%8].split())\n",
    "            c = a.intersection(b)\n",
    "            sim_jaccard_method= float(len(c)) / (len(a) + len(b) - len(c))\n",
    "            jaccard_MDA.append(sim_jaccard_method)\n",
    "        #     print(sim_jaccard_method)\n",
    "\n",
    "        df['jaccard_similarity_mda']=jaccard_MDA\n",
    "\n",
    "        #To find out Cosine Similarity\n",
    "\n",
    "        #Cosine SImilarity MDA Section\n",
    "        Cosine_MDA=['NA']\n",
    "        str=[MDA_FormatText[0],MDA_FormatText[1],MDA_FormatText[2],MDA_FormatText[3],MDA_FormatText[4],MDA_FormatText[5],MDA_FormatText[6],MDA_FormatText[7]]\n",
    "\n",
    "        for i in range(0,7):\n",
    "\n",
    "\n",
    "            vector1 = text_to_vector(str[i%8])\n",
    "            vector2 = text_to_vector(str[(i+1)%8])\n",
    "\n",
    "            cosine = get_cosine(vector1, vector2)\n",
    "            Cosine_MDA.append(cosine)\n",
    "\n",
    "        df['cosine_similarity_mda']=Cosine_MDA\n",
    "\n",
    "        #To find out Negative and Positive word in MDA Section\n",
    "\n",
    "        Count_positive_MDA=[]\n",
    "        Count_negative_MDA=[]\n",
    "        diff_MDA=[]\n",
    "        for text in MDA_FormatText:\n",
    "\n",
    "\n",
    "            positive_number=loadPositive()\n",
    "            negative_number=loadNegative()\n",
    "            # count = Counter(paragraph.split())\n",
    "            count = Counter(text.split())\n",
    "            #print(count.items())\n",
    "\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            for key, val in count.items():\n",
    "                key = key.rstrip('.,?!\\n') # removing possible punctuation signs\n",
    "                if key in positive_number:\n",
    "                    pos += val\n",
    "                if key in negative_number:\n",
    "                    neg += val\n",
    "            Difference_MDA=neg-pos\n",
    "            Count_positive_MDA.append(pos)\n",
    "            Count_negative_MDA.append(neg)\n",
    "            diff_MDA.append(Difference_MDA)\n",
    "\n",
    "        df['count_positive_mda']=Count_positive_MDA\n",
    "        df['count_negative_mda']=Count_negative_MDA\n",
    "        df['difference_mda']=diff_MDA\n",
    "\n",
    "        #To find out total number of words in MDA Section\n",
    "\n",
    "        Total_words_MDA=[]\n",
    "        for fname in MDA_FormatText:\n",
    "\n",
    "            num_words = 0\n",
    "\n",
    "            for line in fname:\n",
    "                words = line.split()\n",
    "                num_words += len(words)\n",
    "            Total_words_MDA.append(num_words)\n",
    "\n",
    "        df['total_words_mda']=Total_words_MDA\n",
    "\n",
    "        df['pessimism_mda']=df['difference_mda']/df['total_words_mda']\n",
    "\n",
    "        #Find out the change in Pessism\n",
    "\n",
    "        df['change_in_pessimism_over_last_year_mda'] = df['pessimism_mda'] - df['pessimism_mda'].shift(-1)\n",
    "\n",
    "\n",
    "\n",
    "        RiskFactor_list=[]\n",
    "        for text in FormatText_New:\n",
    "            rf = parse_rf(text)\n",
    "            RiskFactor_list.append(rf)\n",
    "        #     print(type(rf))\n",
    "            # Parse second time if first parse results in index\n",
    "            if rf and len(rf.encode('utf-8')) < 1000:\n",
    "                rf= parse_rf(text, start=end)\n",
    "\n",
    "        RiskFactor_FormatText=[]\n",
    "        for text in RiskFactor_list:\n",
    "\n",
    "            text = unicodedata.normalize(\"NFKD\", text)  # Normalize\n",
    "            text = '\\n'.join(\n",
    "                text.splitlines())  # Let python take care of unicode break lines\n",
    "\n",
    "            # Convert to upper\n",
    "        #     text = text.upper()  # Convert to upper\n",
    "\n",
    "            # Take care of breaklines & whitespaces combinations due to beautifulsoup parsing\n",
    "            text = re.sub(r'[ ]+\\n', '\\n', text)\n",
    "            text = re.sub(r'\\n[ ]+', '\\n', text)\n",
    "            text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "            # To find MDA section, reformat item headers\n",
    "            text = text.replace('\\n.\\n', ' ')  # Move Period to beginning\n",
    "\n",
    "        # #     text = text.replace('\\nI\\nTEM', '\\nITEM')\n",
    "        #     text = text.replace('\\nI\\ntem', '\\nItem')\n",
    "        # #     text = text.replace('\\nITEM\\n', '\\nITEM ')\n",
    "        #     text= text.replace('\\nItem\\n' ,'\\nItem')\n",
    "        # #     text = text.replace('\\nITEM  ', '\\nITEM ')\n",
    "        #     text= text.replace('\\nItem' , '\\nItem')\n",
    "\n",
    "            text = text.replace(':\\n', ' ')\n",
    "\n",
    "            # Math symbols for clearer looks\n",
    "            text = text.replace('$\\n', '$')\n",
    "            text = text.replace('\\n%', '%')\n",
    "            text = text.replace('\\n', ' ')\n",
    "\n",
    "        #     # Reformat\n",
    "        #     text = text.replace('\\n', '\\n\\n')  # Reformat by additional breakline\n",
    "\n",
    "            RiskFactor_FormatText.append(text)\n",
    "\n",
    "        df['riskfactor_formattext']=RiskFactor_FormatText\n",
    "\n",
    "        #Find out Jaccard Similarity for Risk Factor Section\n",
    "        jaccard_RF=['NA']\n",
    "\n",
    "        str=[RiskFactor_list[0],RiskFactor_list[1],RiskFactor_list[2],RiskFactor_list[3],RiskFactor_list[4],RiskFactor_list[5],RiskFactor_list[6],RiskFactor_list[7]]\n",
    "        for i in range(0,7):\n",
    "\n",
    "            a = set(str[i%8].split())\n",
    "            b = set(str[(i+1)%8].split())\n",
    "            c = a.intersection(b)\n",
    "            sim_jaccard_method= float(len(c)) / (len(a) + len(b) - len(c))\n",
    "            jaccard_RF.append(sim_jaccard_method)\n",
    "        #     print(sim_jaccard_method)\n",
    "\n",
    "\n",
    "        df['jaccard_similarity_rf']=jaccard_RF\n",
    "\n",
    "        #Cosine Similarity for Risk Factor Section\n",
    "\n",
    "        Cosine_RF=['NA']\n",
    "        str=[RiskFactor_FormatText[0],RiskFactor_FormatText[1],RiskFactor_FormatText[2],RiskFactor_FormatText[3],RiskFactor_FormatText[4],RiskFactor_FormatText[5],RiskFactor_FormatText[6],RiskFactor_FormatText[7]]\n",
    "\n",
    "        for i in range(0,7):\n",
    "\n",
    "\n",
    "            vector1 = text_to_vector(str[i%8])\n",
    "            vector2 = text_to_vector(str[(i+1)%8])\n",
    "\n",
    "            cosine = get_cosine(vector1, vector2)\n",
    "            Cosine_RF.append(cosine)\n",
    "\n",
    "\n",
    "        df['cosine_similarity_rf']=Cosine_RF\n",
    "\n",
    "        #Find out Positive and Negative words in RiskFactor Section\n",
    "\n",
    "        Count_positive_RF=[]\n",
    "        Count_negative_RF=[]\n",
    "        diff_RF=[]\n",
    "        for text in RiskFactor_FormatText:\n",
    "\n",
    "\n",
    "            pos_number=loadPositive()\n",
    "            neg_number=loadNegative()\n",
    "            # count = Counter(paragraph.split())\n",
    "            count = Counter(text.split())\n",
    "            #print(count.items())\n",
    "\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            for key, val in count.items():\n",
    "                key = key.rstrip('.,?!\\n') # removing possible punctuation signs\n",
    "                if key in pos_number:\n",
    "                    pos += val\n",
    "                if key in neg_number:\n",
    "                    neg += val\n",
    "            Difference_RF=neg-pos\n",
    "            Count_positive_RF.append(pos)\n",
    "            Count_negative_RF.append(neg)\n",
    "            diff_RF.append(Difference_RF)\n",
    "\n",
    "        df['count_positive_rf']=Count_positive_RF\n",
    "        df['count_negative_rf']=Count_negative_RF\n",
    "        df['difference_rf']=diff_RF\n",
    "\n",
    "        #Find out total words in Risk Factor Section\n",
    "\n",
    "        Total_words_RF=[]\n",
    "        for fname in RiskFactor_FormatText:\n",
    "\n",
    "            num_words = 0\n",
    "\n",
    "            for line in fname:\n",
    "                words = line.split()\n",
    "                num_words += len(words)\n",
    "            Total_words_RF.append(num_words)\n",
    "\n",
    "        df['total_words_rf']=Total_words_RF\n",
    "\n",
    "        df['pessimism_rf']=df['difference_rf']/df['total_words_rf']\n",
    "\n",
    "        #Find the change in Pessimism in Risk Factor Section\n",
    "\n",
    "        df['change_in_pessimism_over_last_year_rf'] = df['pessimism_rf'] - df['pessimism_rf'].shift(-1)\n",
    "\n",
    "        #Importing the daily stock report from yahoo\n",
    "\n",
    "        from pandas_datareader import data, wb\n",
    "        import datetime\n",
    "\n",
    "        # We will look at stock prices over the past year, starting at January 1, 2010\n",
    "        start = datetime.datetime(2010,6,1)\n",
    "        # end = datetime.date.today()\n",
    "        end= datetime.datetime(2018,5, 30)\n",
    "\n",
    "        # First argument is the series we want, second is the source (\"yahoo\" for Yahoo! Finance), third is the start date, fourth is the end date\n",
    "        # Get the stock price data from yahoo for the company \n",
    "        Company_name = data.get_data_yahoo(Company_ticker, start, end) \n",
    "\n",
    "        df_new = pd.DataFrame(data=Company_name)  #Company_Name\n",
    "        df_new=df_new.drop('Adj Close',axis=1)\n",
    "\n",
    "        df_new=df_new.drop('Volume',axis=1)\n",
    "\n",
    "        df_new=df_new.reset_index()\n",
    "        \n",
    "        #Year wise grouping and find out Open,close, high , low for that particular year(June-May)\n",
    "\n",
    "        group=[]\n",
    "        for value in range(df_new['Date'].shape[0]):\n",
    "            curr_year=df_new['Date'].dt.year[value]\n",
    "            curr_month=df_new['Date'].dt.month[value]\n",
    "            curr_day=df_new['Date'].dt.day[value]\n",
    "            group.append(grouping(2010,curr_year,curr_month,curr_day))\n",
    "\n",
    "        df_new['Group Number']=group\n",
    "\n",
    "        low=[]\n",
    "        low_value=df_new.groupby(\"Group Number\").min()[\"Low\"].reset_index()\n",
    "        low.append(low_value)\n",
    "\n",
    "\n",
    "        low=low[::-1]\n",
    "\n",
    "        high=[]\n",
    "        high_value=df_new.groupby(\"Group Number\").max()[\"High\"].reset_index()\n",
    "        high.append(high_value)\n",
    "\n",
    "        high=high[::-1]\n",
    "\n",
    "        df3=df_new.groupby('Group Number').first().reset_index()\n",
    "\n",
    "        open= df3['Open']  #FIND THE OPEN VALUES FOR EACH GROUP\n",
    "\n",
    "        open=open[::-1]\n",
    "\n",
    "        date= df3['Date'] #start date of each quarter\n",
    "\n",
    "        dflast=df_new.groupby('Group Number').last().reset_index()\n",
    "\n",
    "        close= dflast['Close']\n",
    "\n",
    "        close=close[::-1]\n",
    "\n",
    "        date=date[::-1]\n",
    "\n",
    "        df_yearly=pd.DataFrame({'date':date})\n",
    "        df_yearly['open']=open\n",
    "        df_yearly['high']=high_value['High']\n",
    "        df_yearly['low']=low_value['Low']\n",
    "        df_yearly['close']=close\n",
    "\n",
    "        df_yearly.index = range(8)\n",
    "        \n",
    "        #Quarter wise grouping and find out open,close,high ,low for each quarter in that year (June-Aug, Sept-Nov, Dec-Feb ,Mar-May)\n",
    "\n",
    "        df_quart = pd.DataFrame(data=Company_name)\n",
    "\n",
    "        df_quart=df_quart.drop('Adj Close',axis=1)\n",
    "\n",
    "        df_quart=df_quart.drop('Volume',axis=1)\n",
    "\n",
    "        df_quart=df_quart.reset_index()\n",
    "\n",
    "        group1=[]\n",
    "        for value in range(df_quart['Date'].shape[0]):\n",
    "            curr_year=df_quart['Date'].dt.year[value]\n",
    "            curr_month=df_quart['Date'].dt.month[value]\n",
    "            curr_day=df_quart['Date'].dt.day[value]\n",
    "            group1.append(group_quarter(2010,curr_year,curr_month,curr_day))\n",
    "\n",
    "        df_quart['Quarterly Group']=group1\n",
    "\n",
    "        df_quart=df_quart.reset_index()\n",
    "\n",
    "        low_Quarter=[]\n",
    "        low_value=df_quart.groupby(\"Quarterly Group\").min()[\"Low\"].reset_index()\n",
    "        low_Quarter.append(low_value)\n",
    "\n",
    "        low_Quarter=low_Quarter[::-1]\n",
    "\n",
    "        high_Quarter=[]\n",
    "        high_value=df_quart.groupby(\"Quarterly Group\").max()[\"High\"].reset_index()\n",
    "        high_Quarter.append(high_value)\n",
    "\n",
    "        high_Quarter=high_Quarter[::-1]\n",
    "\n",
    "        df6=df_quart.groupby('Quarterly Group').first().reset_index() \n",
    "\n",
    "        open= df6['Open']\n",
    "\n",
    "        open=open[::-1]\n",
    "\n",
    "        df7=df_quart.groupby('Quarterly Group').last().reset_index()\n",
    "\n",
    "        date=df6['Date']\n",
    "        date=date[::-1]\n",
    "        close= df6['Close']\n",
    "        close=close[::-1]\n",
    "\n",
    "        close_value=[]\n",
    "        close_value.append(close)\n",
    "\n",
    "        start_date=[]\n",
    "        start_date.append(date)\n",
    "\n",
    "        df_quarterwise=pd.DataFrame({'date':date})\n",
    "\n",
    "        df_quarterwise['open']=open\n",
    "\n",
    "        df_quarterwise['high']=high_value['High']\n",
    "\n",
    "        df_quarterwise['low']=low_value['Low']\n",
    "        df_quarterwise['close']=close\n",
    "\n",
    "\n",
    "\n",
    "        Number=[]\n",
    "        for value in range(df_quarterwise['date'].shape[0]):\n",
    "            curr_year=df_quarterwise['date'].dt.year[value]\n",
    "            curr_month=df_quarterwise['date'].dt.month[value]\n",
    "            curr_day=df_quarterwise['date'].dt.day[value]\n",
    "            Number.append(Number_quarter(2010,curr_year,curr_month,curr_day))\n",
    "\n",
    "        Number=Number[::-1]\n",
    "\n",
    "        df_quarterwise['Quarter Number']=Number\n",
    "\n",
    "\n",
    "        Year_Quarter=[]\n",
    "        for value in range(df_quarterwise['date'].shape[0]):\n",
    "            curr_year=df_quarterwise['date'].dt.year[value]\n",
    "            curr_month=df_quarterwise['date'].dt.month[value]\n",
    "            curr_day=df_quarterwise['date'].dt.day[value]\n",
    "            Year_Quarter.append(Year(2010,curr_year,curr_month,curr_day))\n",
    "\n",
    "        Year_Quarter=Year_Quarter[::-1]\n",
    "\n",
    "        df_quarterwise['year']=Year_Quarter\n",
    "\n",
    "        df_quarterwise=df_quarterwise.drop('date',axis=1)\n",
    "\n",
    "        df_quarterwise.index=range(32)\n",
    "\n",
    "        df_final=df_quarterwise.pivot_table(index=[\"year\"][::-1],columns=[\"Quarter Number\"],values=[\"open\",\"high\",\"low\",\"close\"])\n",
    "\n",
    "        df_quarterfinal=df_final.swaplevel(0, 1, 1).sort_index(1)\n",
    "\n",
    "        df_quarterfinal=df_quarterfinal[::-1]\n",
    "\n",
    "        df_quarterfinal.index = range(8)\n",
    "        \n",
    "        #datafrane which concats the scrape out section of the SEC Filing and the analysis, yearly stock price (June-May), Quarter-wise stock price (June-Aug, Sept-Nov, Dec-Feb ,Mar-May)\n",
    "\n",
    "        final=pd.concat([df,df_yearly,df_quarterfinal],axis=1)\n",
    "        \n",
    "        #Renaming the column name \n",
    "\n",
    "        fin_table=final.rename(columns={ final.columns[34]: \"Q1_close\",final.columns[35]:\"Q1_high\",final.columns[36]:\"Q1_low\", final.columns[37]:\"Q1_open\",final.columns[38]:\"Q2_close\",final.columns[39]:\"Q2_high\",final.columns[40]:\"Q2_low\",final.columns[41]:\"Q2_open\",final.columns[42]:\"Q3_close\",final.columns[43]:\"Q3_high\",final.columns[44]:\"Q3_low\",final.columns[45]:\"Q3_open\",final.columns[46]:\"Q4_close\",final.columns[47]:\"Q4_high\",final.columns[48]:\"Q4_low\",final.columns[49]:\"Q4_open\"})\n",
    "\n",
    "        return fin_table\n",
    "        # return the dataframe consiting of all the details for a company (Scraping section ,Finding cosine and jacarrd similarity, pessimism change as well as the stock price for that particular year )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r=edgar_data(\"NTAP\",\"NetApp, Inc.\",\"0001002047\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  \\n10-K\\n1\\nntap-10k_20180427.htm\\n10-K\\n\\n\\n\\n...   \n",
      "1  \\n10-K\\n1\\nntap-10k_20170428.htm\\n10-K\\n\\n\\n\\n...   \n",
      "2  \\n10-K\\n1\\nntap-10k_20160429.htm\\n10-K\\n\\n\\n\\n...   \n",
      "3  \\n10-K\\n1\\nntap-10k_20150424.htm\\n10-K\\n\\n\\n\\n...   \n",
      "4  \\n10-K\\n1\\nd691214d10k.htm\\n10-K\\n\\n\\n10-K\\n\\n...   \n",
      "5  \\n10-K\\n1\\nd512428d10k.htm\\nFORM 10-K\\n\\n\\nFor...   \n",
      "6  \\n10-K\\n1\\nd328654d10k.htm\\nFORM 10-K\\n\\n\\nFor...   \n",
      "7  \\n10-K\\n1\\nd10k.htm\\nFORM 10-K\\n\\n\\nForm 10-K\\...   \n",
      "\n",
      "                                         format_text  \\\n",
      "0   10-K 1 ntap-10k_20180427.htm 10-K ntap-10k_20...   \n",
      "1   10-K 1 ntap-10k_20170428.htm 10-K ntap-10k_20...   \n",
      "2   10-K 1 ntap-10k_20160429.htm 10-K ntap-10k_20...   \n",
      "3   10-K 1 ntap-10k_20150424.htm 10-K ntap-10k_20...   \n",
      "4   10-K 1 d691214d10k.htm 10-K 10-K Table of Con...   \n",
      "5   10-K 1 d512428d10k.htm FORM 10-K Form 10-K Ta...   \n",
      "6   10-K 1 d328654d10k.htm FORM 10-K Form 10-K Ta...   \n",
      "7   10-K 1 d10k.htm FORM 10-K Form 10-K Table of ...   \n",
      "\n",
      "  jaccard_similarity_whole_text  cosine_similarity_whole_text  \\\n",
      "0                             NA                           NA   \n",
      "1                       0.774567                     0.995075   \n",
      "2                       0.781481                     0.994767   \n",
      "3                       0.662066                     0.986449   \n",
      "4                       0.677638                     0.985311   \n",
      "5                       0.734017                     0.992748   \n",
      "6                       0.652697                     0.991527   \n",
      "7                       0.714433                      0.99367   \n",
      "\n",
      "   count_positive_fulltext  count_negative_fulltext  difference_fulltext  \\\n",
      "0                      357                      839                  482   \n",
      "1                      331                      778                  447   \n",
      "2                      365                      791                  426   \n",
      "3                      337                      769                  432   \n",
      "4                      311                      787                  476   \n",
      "5                      329                      803                  474   \n",
      "6                      382                     1040                  658   \n",
      "7                      375                     1089                  714   \n",
      "\n",
      "   total_words_fulltext  pessimism  change_in_pessimism_over_last_year  \\\n",
      "0                300814   0.001602                            0.000064   \n",
      "1                290498   0.001539                            0.000113   \n",
      "2                298751   0.001426                           -0.000077   \n",
      "3                287410   0.001503                           -0.000081   \n",
      "4                300489   0.001584                            0.000060   \n",
      "5                311103   0.001524                           -0.000322   \n",
      "6                356570   0.001845                           -0.000084   \n",
      "7                370082   0.001929                                 NaN   \n",
      "\n",
      "     ...         Q2_low    Q2_open   Q3_close    Q3_high     Q3_low  \\\n",
      "0    ...      37.549999  38.820000  56.410000  64.059998  52.000000   \n",
      "1    ...      30.360001  34.740002  35.439999  42.180000  34.720001   \n",
      "2    ...      28.750000  31.070000  30.969999  31.639999  20.660000   \n",
      "3    ...      37.439999  42.099998  41.980000  43.580002  35.880001   \n",
      "4    ...      38.340000  41.619999  41.000000  45.959999  39.000000   \n",
      "5    ...      26.260000  34.369999  32.139999  37.020000  31.709999   \n",
      "6    ...      33.000000  37.790001  36.919998  44.139999  33.500000   \n",
      "7    ...      40.740002  41.009998  52.189999  61.020000  49.820000   \n",
      "\n",
      "     Q3_open   Q4_close    Q4_high     Q4_low    Q4_open  \n",
      "0  55.590000  60.599998  72.849998  58.759998  60.630001  \n",
      "1  36.160000  43.009998  43.139999  39.060001  42.020000  \n",
      "2  30.290001  25.540001  27.510000  22.500000  25.040001  \n",
      "3  42.060001  38.580002  38.930000  30.850000  38.290001  \n",
      "4  40.500000  40.500000  41.029999  33.340000  40.099998  \n",
      "5  31.820000  33.950001  39.150002  32.750000  33.810001  \n",
      "6  36.720001  43.310001  46.799999  27.790001  43.130001  \n",
      "7  51.509998  50.630001  56.490002  44.500000  51.930000  \n",
      "\n",
      "[8 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading two excel sheet (One containing the list of tickers and the other containing the Ticker, Company Name and CIK Number)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df7=pd.read_excel('/apps/apps1/a625533/tick.xlsx')\n",
    "df8=pd.read_excel('/apps/apps1/a625533/fin.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding out the company name and CIK number for the list of tickers for which we want the required datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results=df8.merge(df7,on='Ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results=results.drop('Unnamed: 2',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Name</th>\n",
       "      <th>CIK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>Applied Optoelectronics Inc</td>\n",
       "      <td>1158114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AKS</td>\n",
       "      <td>Ak Steel Holding Corp</td>\n",
       "      <td>918160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMC</td>\n",
       "      <td>American Mortgage Acceptance Co</td>\n",
       "      <td>878774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMC</td>\n",
       "      <td>Amc Entertainment Holdings Inc</td>\n",
       "      <td>1411579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMD</td>\n",
       "      <td>Advanced Micro Devices Inc</td>\n",
       "      <td>2488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AMPE</td>\n",
       "      <td>American Petroleum Group Inc</td>\n",
       "      <td>1138593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AMPE</td>\n",
       "      <td>Ampio Pharmaceuticals Inc</td>\n",
       "      <td>1411906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AN</td>\n",
       "      <td>Autonation Inc</td>\n",
       "      <td>350698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANCX</td>\n",
       "      <td>Access National Corp</td>\n",
       "      <td>1176316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AR</td>\n",
       "      <td>Antero Resources Corp</td>\n",
       "      <td>1433270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AREX</td>\n",
       "      <td>Approach Resources Inc</td>\n",
       "      <td>1405073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ARII</td>\n",
       "      <td>American Railcar Industries Inc</td>\n",
       "      <td>1344596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ASPS</td>\n",
       "      <td>Altisource Portfolio Solutions SA</td>\n",
       "      <td>1462418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ATNM</td>\n",
       "      <td>Actinium Pharmaceuticals Inc</td>\n",
       "      <td>1388320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AXDX</td>\n",
       "      <td>Accelerate Diagnostics Inc</td>\n",
       "      <td>727207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BANC</td>\n",
       "      <td>Banc Of California Inc</td>\n",
       "      <td>1169770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BBOX</td>\n",
       "      <td>Black Box Corp</td>\n",
       "      <td>849547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BEBE</td>\n",
       "      <td>Bebe Stores Inc</td>\n",
       "      <td>1059272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BGFV</td>\n",
       "      <td>Big 5 Sporting Goods Corp</td>\n",
       "      <td>1156388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BKE</td>\n",
       "      <td>Buckle Inc</td>\n",
       "      <td>885245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ticker                               Name      CIK\n",
       "0    AAOI        Applied Optoelectronics Inc  1158114\n",
       "1     AKS              Ak Steel Holding Corp   918160\n",
       "2     AMC    American Mortgage Acceptance Co   878774\n",
       "3     AMC     Amc Entertainment Holdings Inc  1411579\n",
       "4     AMD         Advanced Micro Devices Inc     2488\n",
       "5    AMPE       American Petroleum Group Inc  1138593\n",
       "6    AMPE          Ampio Pharmaceuticals Inc  1411906\n",
       "7      AN                     Autonation Inc   350698\n",
       "8    ANCX               Access National Corp  1176316\n",
       "9      AR              Antero Resources Corp  1433270\n",
       "10   AREX             Approach Resources Inc  1405073\n",
       "11   ARII    American Railcar Industries Inc  1344596\n",
       "12   ASPS  Altisource Portfolio Solutions SA  1462418\n",
       "13   ATNM       Actinium Pharmaceuticals Inc  1388320\n",
       "14   AXDX         Accelerate Diagnostics Inc   727207\n",
       "15   BANC             Banc Of California Inc  1169770\n",
       "16   BBOX                     Black Box Corp   849547\n",
       "17   BEBE                    Bebe Stores Inc  1059272\n",
       "18   BGFV          Big 5 Sporting Goods Corp  1156388\n",
       "19    BKE                         Buckle Inc   885245"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Iterating over 50 tickers and checking for how many companies we are able to get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRR\n",
      "DO\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for index, row in results.head(50).iterrows():\n",
    "    \n",
    "    Company_ticker=row['Ticker']\n",
    "    Company_Name=row['Name']\n",
    "    CIK_Number=str(row['CIK'])\n",
    "    \n",
    "    \n",
    "    try:\n",
    "    \n",
    "        r=edgar_data(Company_ticker, Company_Name, CIK_Number)\n",
    "        c+=1\n",
    "        print(row['Ticker'])\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rslt_df = results.loc[results['Ticker']=='CRR'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Name</th>\n",
       "      <th>CIK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>CRR</td>\n",
       "      <td>Carbo Ceramics Inc</td>\n",
       "      <td>1009672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ticker                Name      CIK\n",
       "33    CRR  Carbo Ceramics Inc  1009672"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "O=edgar_data(\"CRR\",\"Carbo Ceramics Inc\",\"1009672\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  \\n10-K\\n1\\ncrr-10k_20181231.htm\\n10-K\\n\\n\\n\\n\\...   \n",
      "1  \\n10-K\\n1\\ncrr-10k_20171231.htm\\n10-K\\n\\n\\n\\n\\...   \n",
      "2  \\n10-K\\n1\\ncrr-10k_20161231.htm\\n10-K\\n\\n\\n\\n\\...   \n",
      "3  \\n10-K\\n1\\nd104557d10k.htm\\nFORM 10-K\\n\\n\\nFor...   \n",
      "4  \\n10-K\\n1\\nd839115d10k.htm\\n10-K\\n\\n\\n10-K\\n\\n...   \n",
      "5  \\n10-K\\n1\\nd637538d10k.htm\\n10-K\\n\\n\\n10-K\\n\\n...   \n",
      "6  \\n10-K\\n1\\nd439385d10k.htm\\nFORM 10-K\\n\\n\\nFOR...   \n",
      "7  \\n10-K\\n1\\nd238954d10k.htm\\nFORM 10-K\\n\\n\\nFor...   \n",
      "\n",
      "                                         format_text  \\\n",
      "0   10-K 1 crr-10k_20181231.htm 10-K crr-10k_2018...   \n",
      "1   10-K 1 crr-10k_20171231.htm 10-K crr-10k_2017...   \n",
      "2   10-K 1 crr-10k_20161231.htm 10-K crr-10k_2016...   \n",
      "3   10-K 1 d104557d10k.htm FORM 10-K Form 10-K Ta...   \n",
      "4   10-K 1 d839115d10k.htm 10-K 10-K UNITED STATE...   \n",
      "5   10-K 1 d637538d10k.htm 10-K 10-K Table of Con...   \n",
      "6   10-K 1 d439385d10k.htm FORM 10-K FORM 10-K Ta...   \n",
      "7   10-K 1 d238954d10k.htm FORM 10-K Form 10-K Ta...   \n",
      "\n",
      "  jaccard_similarity_whole_text  cosine_similarity_whole_text  \\\n",
      "0                             NA                           NA   \n",
      "1                        0.70655                     0.993086   \n",
      "2                       0.745738                     0.995127   \n",
      "3                       0.694019                     0.994079   \n",
      "4                       0.716118                     0.981405   \n",
      "5                       0.759448                     0.995283   \n",
      "6                       0.787238                     0.997217   \n",
      "7                       0.774273                     0.996769   \n",
      "\n",
      "   count_positive_fulltext  count_negative_fulltext  difference_fulltext  \\\n",
      "0                      174                      481                  307   \n",
      "1                      197                      620                  423   \n",
      "2                      183                      609                  426   \n",
      "3                      158                      624                  466   \n",
      "4                      118                      370                  252   \n",
      "5                      120                      382                  262   \n",
      "6                      111                      373                  262   \n",
      "7                      108                      344                  236   \n",
      "\n",
      "   total_words_fulltext  pessimism  change_in_pessimism_over_last_year  \\\n",
      "0                193545   0.001586                           -0.000441   \n",
      "1                208698   0.002027                            0.000040   \n",
      "2                214422   0.001987                           -0.000411   \n",
      "3                194384   0.002397                            0.000842   \n",
      "4                162017   0.001555                           -0.000097   \n",
      "5                158585   0.001652                           -0.000026   \n",
      "6                156099   0.001678                            0.000140   \n",
      "7                153438   0.001538                                 NaN   \n",
      "\n",
      "      ...         Q2_low     Q2_open    Q3_close     Q3_high      Q3_low  \\\n",
      "0     ...       6.100000    6.560000   10.350000   12.690000    6.650000   \n",
      "1     ...       5.660000   12.410000    8.140000   16.700001    7.920000   \n",
      "2     ...      15.510000   26.219999   18.690001   20.330000   13.210000   \n",
      "3     ...      38.009998  107.519997   37.000000   43.430000   27.969999   \n",
      "4     ...      81.010002   82.699997  124.800003  126.470001  104.080002   \n",
      "5     ...      60.330002   70.250000   78.059998   93.430000   73.269997   \n",
      "6     ...      90.410004  159.949997  144.009995  147.009995   85.680000   \n",
      "7     ...      76.120003   76.459999  100.720001  124.790001   95.320000   \n",
      "\n",
      "      Q3_open    Q4_close     Q4_high      Q4_low     Q4_open  \n",
      "0   10.200000    7.140000   11.860000    6.730000    6.800000  \n",
      "1    8.800000   13.550000   13.870000    6.210000   13.030000  \n",
      "2   18.719999   19.670000   24.200001   10.600000   20.559999  \n",
      "3   37.930000   36.189999   44.849998   29.350000   36.470001  \n",
      "4  123.559998  122.889999  144.350006  117.250000  122.510002  \n",
      "5   77.290001   90.199997   97.860001   65.809998   90.809998  \n",
      "6  141.490005   88.839996  110.660004   79.540001   91.470001  \n",
      "7   98.940002  117.680000  169.339996  112.220001  124.010002  \n",
      "\n",
      "[8 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "print(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "for index, row in results.iterrows():\n",
    "    \n",
    "    Company_ticker=row['Ticker']\n",
    "    Company_Name=row['Name']\n",
    "    CIK_Number=str(row['CIK'])\n",
    "    \n",
    "    \n",
    "    try:\n",
    "    \n",
    "        r=edgar_data(Company_ticker, Company_Name, CIK_Number)\n",
    "        c+=1\n",
    "        print(row['Ticker'])\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
